---
layout: post
title: CEE491 Lecture 01-05
categories: CEE491:Decision-and-Risk-Analysis
description: Personal Notes
keywords: CEE491，Risk-Analysis，Probability, Decision
mathjax: true
---

# L1
> **Types of uncertainties**

- Inherent variability or randomness
- Statistical uncertainty
- Measurement error
- Model error
- Human error

Statistical uncertatinty could be reduced through accumulation of data.

## 1.1 Notation

**Sample Space** $S$: the collection of all possible outcomes. 
**Sample Point** $x$: each indivisual outcome
**Event** $E$: any collection of sample points

General relation: $x\in S$, $E\subseteq S$
Special case: $E=S$: certain event; $E=\empty$: null event

# L2
## 2.1 Sets of Events
Mutually exclusive events: $E_1E_2=\empty$

Complementary events: $E_1\overline{E_2}=\emptyset$

Collective exhaustive events: $\bigcup_{i=1}^M  E_i=S$

## 2.2 Associate Properties

(1) $(E_1\cup E_2)\cup E_3 = E_1\cup(E_2 \cup E_3) = E_1\cup E_2\cup E_3$

(2) $(E_1E_2)E_3 = E_1(E_2E_3) = E_1E_2E_3$

## 2.3 De Morgan's Rules

$$\tag{2.1}\overline{\bigcup_{i=1}^M  E_i}=\bigcap_{i=1}^M \overline{E_i}$$

That is $\overline{E_i\cup E_2\cup...\cup E_M}=\overline{E_1}...\overline{E_M}$

$$\tag{2}\overline{\bigcap_{i=1}^M  E_i}=\bigcup_{i=1}^M \overline{E_i}$$

That is $\overline{E_1...E_M}=\overline{E_1}\cup\overline{E_1}\cup\overline{E_M}$

> **Representation through circuits**

Let $E_i$ indicates the failure event of $i$-th component, and $E_{sys}$ indicates the whole system's failure. The parallel and series circuits are shown below:

<center>
    <img src="/images/2021-09/011300.jpg" style="zoom:40%"> <br><div style="color: #999;"></div>
</center><br>

For series connections: $E_{sys} = E_1\cup E_2\cup E_3$
$\overline{E_{sys}}=\bar{E_1}\bar{E_2}\bar{E_3}$

For parallel connections: $E_{sys} = E_1E_2E_3$
$\overline{E_{sys}}=\bar{E_1}\cup\bar{E_2}\cup\bar{E_3}$

# L3
## 3.1 Elements of Probability Theory
$P(E)$: likelihood of occurrence of event $E$ in sample space relative to the other evenets

> **Axioms:**
(1) $P(E)\geq 0$
(2) $P(S)=1$
(3) $P(E_1\cup E_2)=P(E_1) + P(E_2)$ if mutually exclusive

Using $S=\bar{E}\cup E$, and axiom 2 and 3: $P(\bar{E})= 1-P(E)$

#### Inclusion-Exclusion Rule
For simplest case: $P(E_1\cup E_2)=P(E_1) + P(E_2) - P(E_1E_2)$

For general cases:
$$P(\bigcup_{i=1}^nE_i)=\sum_{i=1}^nP(E_i) - \sum_{i=1}^{n-1}\sum_{j=i+1}^nP(E_iE_j)+...+(-1)^{n-1}P(E_1E_2...E_n)$$

#### Conditional Probability

$$P(E_1\vert E_2)=\begin{cases}
   \frac{P(E_1E_2)}{P(E_2)} & P(E_2)\neq 0\\
   0 & P(E_2)= 0
\end{cases}$$

#### Multiplication Rule
$$P(E_1E_2)=P(E_1\vert E_2)\cdot P(E_2)$$

#### Statistical Independence

Two events are **statistically independant (S.I.)** if the occurrence of one event doesn't change the probability of the other, that is, $P(E_1\vert E_2)=P(E_1)$ and $P(E_2\vert E_1)=P(E_2)$. 

Only when $E_1,E_2$ are S.I., the formulation $P(E_1E_2)=P(E_1)\cdot P(E_2)$ works

# L4
<span style="background-color: yellow; color: black;">There is an important example written in ipad, don't forget to review it.</span>

## 4.1 Bayes's Rule

$$P(E_1\vert E_2)=\frac{P(E_2\vert E_1)}{P(E_2)}\cdot P(E_1)$$

$P(E_1\vert E_2)$ posterior probability
$P(E_1)$ prior probability
$P(E_2\vert E_1)$ likelihood
$P(E_2)$ normalizing factor

# L5
## 5.1 Total Probability Rule

Let $E_1,E_2,...,E_M$:

(1) $E_iE_i=\empty$ for any $i\neq j$  
(2) $\bigcup_{i=1}^ME_i=S$

then
$P(A)=\sum_{i=1}^MP(A\vert E_i)P(E_i)$


That's because $=\sum_{i=1}^MP(AE_i)=P(AE_1\cup...\cup AE_M)=P(A(E_1\cup...\cup E_M))=P(AS)=P(A)$







